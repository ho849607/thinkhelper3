# -*- coding: utf-8 -*-
"""
ThinkHelper Brain v2
- ì‹¤ì‹œê°„(on-type) í•™ìŠµ + ì¦‰ì‹œ ì¶”ì²œ
- ì „ì—­/ë¬¸ì„œë³„ ìˆ˜ë½ ì¹´ìš´íŠ¸(ê°•í™”í•™ìŠµ) + ì‹œê°„ê°ì‡ 
- ë¬¸ì„œë³„ TF(ë¹ˆë„) ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ê°€ì¤‘
- ì‚¬ìš©ì ì‚¬ì „(user_dict) ë™ê¸°í™” + ê¸°ë³¸ ì‚¬ì „(seed) ê²°í•©
- JSON ì˜ì†í™” (ìŠ¤í‚¤ë§ˆ ë³€í™”ì— ëŒ€ë¹„í•œ ì•ˆì „ ë¡œë“œ)
"""

import re
import time
import json
import math
import os
from collections import Counter
from typing import List, Dict, Optional


def now_ms() -> int:
    return int(time.time() * 1000)


def days_since(ts_ms: Optional[int]) -> float:
    if not ts_ms:
        return 9999.0
    # í˜„ì¬(ms) - ê³¼ê±°(ms) -> ì¼ìˆ˜
    return max(0.0, (now_ms() - ts_ms) / (1000.0 * 60 * 60 * 24))


class ThinkHelperBrain:
    """
    ì‚¬ìš© íŒ¨í„´:
      brain = ThinkHelperBrain()
      brain.observe_text_incremental(doc_id, current_text)   # íƒ€ì´í•‘ ì¤‘ ìˆ˜ì‹œ í˜¸ì¶œ
      cands = brain.get_suggestions(prefix, doc_id, top_n=8) # ì ‘ë‘ì‚¬ ì¶”ì²œ
      brain.accept_suggestion(doc_id, chosen_word)           # íƒ­/ì—”í„° ìˆ˜ë½ì‹œ í˜¸ì¶œ
    """

    def __init__(self, storage_file: str = "brain_data.json"):
        self.storage_file = storage_file

        # ë¶ˆìš©ì–´ (ë„ˆë¬´ í”í•œ ë‹¨ì–´ ì œì™¸)
        self.stop_ko: set = {
            "ê·¸ë¦¬ê³ ", "ê·¸ëŸ¬ë‚˜", "í•˜ì§€ë§Œ", "ë˜", "ë˜ëŠ”", "ë°", "ìˆë‹¤",
            "í•˜ëŠ”", "ì—ì„œ", "ìœ¼ë¡œ", "ì—ê²Œ", "ì…ë‹ˆë‹¤", "ìˆ˜", "ê²ƒ", "ë“±", "ë•Œ"
        }
        self.stop_en: set = {
            "the", "and", "for", "with", "that", "this", "from",
            "have", "are", "not", "to", "of", "in", "on", "by", "as", "be", "is"
        }

        # ê¸°ë³¸(ì‹œë“œ) ì‚¬ì „ â€” í•„ìš”ì‹œ ì¶”ê°€
        self.dict_ko_base: List[str] = [
            "ë¶„ì„","ì—°êµ¬","ê²°ê³¼","ë°©ë²•","ê³¼ì •","ê²°ë¡ ","ì°¸ê³ ","ì¶œì²˜","ë°ì´í„°","ìš”ì•½",
            "ì¢…í•©","í•œê³„","ì „ë§","ìŸì ","ë²•ë¦¬","íŒë¡€","í˜•ë²•","í˜•ì‚¬ì†Œì†¡ë²•","ëŒ€ë²•ì›","ìš”ì§€",
            "ì‚¬ì‹¤ê´€ê³„","ì•Œê³ ë¦¬ì¦˜","í”„ë¡œí† íƒ€ì…","ì»¨í…ìŠ¤íŠ¸","í† í°","ì„ë² ë”©","ì˜¤í”„ë¼ì¸",
            "ìºì‹œ","ìŠ¤ë ˆë“œ","ë¹„ë™ê¸°","ì´ë²¤íŠ¸ë£¨í”„","ë Œë”ë§","ì„±ëŠ¥ìµœì í™”"
        ]
        self.dict_en_base: List[str] = [
            "analysis","baseline","benchmark","context","dataset","design","embedding",
            "evaluation","fallback","feature","guide","heuristic","insight","journey",
            "knowledge","latency","model","note","optimize","pipeline","quality",
            "research","summary","template","validation","workflow","yield","zero-copy"
        ]

        # ìµœëŒ€ ì‚¬ìš©ì ì‚¬ì „ ìš©ëŸ‰(ì–¸ì–´ë³„)
        self.user_dict_caps = {"ko": 400, "en": 400}

        # ë©”ëª¨ë¦¬ ë¡œë“œ + ìŠ¤í‚¤ë§ˆ ë³´ì •
        self.memory = self._load_or_init()

        # ê°ì‡  íŒŒë¼ë¯¸í„°(ì¼ ë‹¨ìœ„)
        self.decay_daily = 0.99

        # ë¬¸ì„œë³„ ìˆ˜ë½ ë¡œê·¸(ì„ íƒì‚¬í•­) â€” í•„ìš”ì‹œ ë¶„ì„ìš©
        # {"doc_id": {"accept_counts": {...}, "last_used_at": {...}}}
        if "per_doc_accept" not in self.memory:
            self.memory["per_doc_accept"] = {}

    # ------------------ Persistence ------------------

    def _load_or_init(self) -> Dict:
        if not os.path.exists(self.storage_file):
            return {
                "accept_counts": {},        # ì „ì—­ ì„ íƒ íšŸìˆ˜ {word: count}
                "last_used_at": {},         # ì „ì—­ ë§ˆì§€ë§‰ ì‚¬ìš© ì‹œê°(ms) {word: ts}
                "doc_freq": {},             # ë¬¸ì„œë³„ TF {doc_id: {word: count}}
                "user_dict": {"ko": [], "en": []},  # ì‚¬ìš©ì ì‚¬ì „
            }
        try:
            with open(self.storage_file, "r", encoding="utf-8") as f:
                mem = json.load(f)
        except Exception:
            # íŒŒì¼ ê¹¨ì¡Œì„ ë•Œ ë³µêµ¬
            mem = {}

        # ìŠ¤í‚¤ë§ˆ ë³´ì •
        mem.setdefault("accept_counts", {})
        mem.setdefault("last_used_at", {})
        mem.setdefault("doc_freq", {})
        mem.setdefault("user_dict", {"ko": [], "en": []})
        # íƒ€ì… ë³´ì •
        for lang in ("ko", "en"):
            if not isinstance(mem["user_dict"].get(lang, []), list):
                mem["user_dict"][lang] = []
        return mem

    def save_memory(self) -> None:
        tmp_path = self.storage_file + ".tmp"
        with open(tmp_path, "w", encoding="utf-8") as f:
            json.dump(self.memory, f, ensure_ascii=False, indent=2)
        os.replace(tmp_path, self.storage_file)

    # ------------------ Tokenization ------------------

    def extract_tokens(self, text: str) -> Dict[str, List[str]]:
        """
        í…ìŠ¤íŠ¸ì—ì„œ ì–¸ì–´ë³„ í† í° ëª©ë¡ ì¶”ì¶œ.
        - í•œê¸€: 2ê¸€ì ì´ìƒ
        - ì˜ì–´: 3ê¸€ì ì´ìƒ, ì†Œë¬¸ìí™”
        - ë¶ˆìš©ì–´ ì œê±°
        """
        text = text or ""
        ko_tokens = re.findall(r"[ê°€-í£]{2,}", text)
        ko_tokens = [w for w in ko_tokens if w not in self.stop_ko and len(w) <= 20]

        en_tokens = re.findall(r"[A-Za-z][A-Za-z\-]{2,}", text)
        en_tokens = [w.lower() for w in en_tokens if w.lower() not in self.stop_en and len(w) <= 24]

        return {"ko": ko_tokens[:1000], "en": en_tokens[:1000]}

    # ------------------ Learning ------------------

    def observe_text_incremental(self, doc_id: str, current_text: str) -> None:
        """
        í˜„ì¬ ë¬¸ì„œì˜ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ë„£ì–´ì£¼ë©´ TFë¥¼ ê°±ì‹ í•˜ê³ ,
        ì‚¬ìš©ì ì‚¬ì „ì„ ìµœì‹  ë¹ˆë„ë¡œ ë™ê¸°í™”í•œë‹¤.
        - í”„ëŸ°íŠ¸ì—”ë“œ: í‚¤ ì…ë ¥(debounce 500~1200ms ê¶Œì¥) ë•Œë§ˆë‹¤ í˜¸ì¶œ
        """
        tokens = self.extract_tokens(current_text)
        all_tokens = tokens["ko"] + tokens["en"]
        self.memory["doc_freq"][doc_id] = dict(Counter(all_tokens))

        # ì‚¬ìš©ì ì‚¬ì „ ë™ê¸°í™” (ê° ì–¸ì–´ë³„ ë¹ˆë„ ìƒìœ„ + ì¤‘ë³µ ì œê±° + ìš©ëŸ‰ ìº¡)
        for lang in ("ko", "en"):
            freq = Counter(tokens[lang])
            # ë…¸ì´ì¦ˆ ì œê±°: 2íšŒ ì´ìƒ ë“±ì¥
            cand = [w for w, c in freq.items() if c >= 2]
            current: set = set(self.memory["user_dict"].get(lang, []))
            # ë§ì´ ë‚˜ì˜¨ ê²ƒë¶€í„° ì¶”ê°€
            for w, _ in freq.most_common():
                if w in current:
                    continue
                if w in cand:
                    current.add(w)
                if len(current) >= self.user_dict_caps[lang]:
                    break
            # ê¸°ë³¸ ì‚¬ì „ê³¼ ì¶©ëŒ ì—†ì´ ìœ ì§€(ì¤‘ë³µ í—ˆìš© X)
            self.memory["user_dict"][lang] = list(current)[: self.user_dict_caps[lang]]

        self.save_memory()

    # ------------------ Scoring ------------------

    def _decay_score(self, count: int, last_ts_ms: Optional[int]) -> float:
        d = days_since(last_ts_ms)  # ì¼ìˆ˜
        return (count or 0) * (self.decay_daily ** d)

    def _context_score(self, word: str, doc_id: Optional[str]) -> float:
        if not doc_id:
            return 0.0
        tf = self.memory["doc_freq"].get(doc_id, {}).get(word, 0)
        # ë¬¸ì„œ ë‚´ ìì£¼ ë“±ì¥í• ìˆ˜ë¡ ê°€ì‚°ì (ìƒí•œ ì™„ë§Œ)
        return 0.2 * min(5, tf)

    def _per_doc_accept_score(self, word: str, doc_id: Optional[str]) -> float:
        if not doc_id:
            return 0.0
        drec = self.memory.get("per_doc_accept", {}).get(doc_id, {})
        cnt = drec.get("accept_counts", {}).get(word, 0)
        last_ts = drec.get("last_used_at", {}).get(word, 0)
        return 1.2 * self._decay_score(cnt, last_ts)

    def _score_word(self, word: str, doc_id: Optional[str]) -> float:
        # ì „ì—­ ê°•í™” ì ìˆ˜(ê°ì‡ )
        g = self._decay_score(
            self.memory["accept_counts"].get(word, 0),
            self.memory["last_used_at"].get(word)
        )
        # ë¬¸ì„œë³„ ê°•í™” + ì»¨í…ìŠ¤íŠ¸ TF ì ìˆ˜
        return g + self._per_doc_accept_score(word, doc_id) + self._context_score(word, doc_id)

    # ------------------ Suggestion ------------------

    def _lang_of_prefix(self, prefix: str) -> str:
        return "ko" if re.search(r"[ê°€-í£]", prefix) else "en"

    def _candidate_pool(self, lang: str) -> List[str]:
        base = self.dict_ko_base if lang == "ko" else self.dict_en_base
        user = self.memory["user_dict"].get(lang, [])
        # ìˆœì„œ ë³´ì¡´í•˜ë©° ì¤‘ë³µ ì œê±°
        seen = set()
        out = []
        for w in user + base:
            if w not in seen:
                seen.add(w)
                out.append(w)
        return out

    def get_suggestions(self, prefix: str, doc_id: Optional[str] = None, top_n: int = 8) -> List[str]:
        if not prefix:
            return []
        lang = self._lang_of_prefix(prefix)
        p = prefix.lower()

        pool = self._candidate_pool(lang)
        cand = [w for w in pool if w.lower().startswith(p)]

        scored = sorted(cand, key=lambda w: (-self._score_word(w, doc_id), w))
        return scored[:top_n]

    # ------------------ Reinforcement ------------------

    def accept_suggestion(self, doc_id: str, word: str) -> None:
        # ì „ì—­ ê°•í™”
        self.memory["accept_counts"][word] = self.memory["accept_counts"].get(word, 0) + 1
        self.memory["last_used_at"][word] = now_ms()

        # ë¬¸ì„œë³„ ê°•í™”
        pda = self.memory.setdefault("per_doc_accept", {})
        drec = pda.setdefault(doc_id, {"accept_counts": {}, "last_used_at": {}})
        drec["accept_counts"][word] = drec["accept_counts"].get(word, 0) + 1
        drec["last_used_at"][word] = now_ms()

        self.save_memory()
        # ë¡œê·¸ ìš©ë„: ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„  ë¡œê¹… ì‹œìŠ¤í…œìœ¼ë¡œ ì „ì†¡
        print(f"ğŸ‘ Learned: '{word}' (global={self.memory['accept_counts'][word]}, doc={drec['accept_counts'][word]})")


# ------------------ Demo ------------------
if __name__ == "__main__":
    brain = ThinkHelperBrain()

    doc_id = "doc_123"
    text = """
    ììœ¨ì£¼í–‰ ê¸°ìˆ ì˜ í•µì‹¬ì€ ì¸ê³µì§€ëŠ¥ê³¼ ì„¼ì„œ í“¨ì „ì´ë‹¤.
    ììœ¨ì£¼í–‰ ìë™ì°¨ëŠ” ë¼ì´ë‹¤ ì„¼ì„œë¥¼ í†µí•´ ì£¼ë³€ì„ ì¸ì‹í•˜ê³ ,
    ì¸ê³µì§€ëŠ¥ ì•Œê³ ë¦¬ì¦˜ì€ íŒë‹¨ì„ ë‚´ë¦°ë‹¤. Analysis and embedding pipeline
    with offline cache and context map.
    """

    print("[1] observe_text_incremental() â€” ì‹¤ì‹œê°„ í•™ìŠµ")
    brain.observe_text_incremental(doc_id, text)

    print("[2] ì ‘ë‘ì‚¬ 'ì' ì¶”ì²œ:", brain.get_suggestions("ì", doc_id, top_n=6))
    print("[3] ì ‘ë‘ì‚¬ 'ana' ì¶”ì²œ:", brain.get_suggestions("ana", doc_id, top_n=6))

    print("[4] ì‚¬ìš©ìê°€ 'ììœ¨ì£¼í–‰' ìˆ˜ë½(ê°•í™”)")
    brain.accept_suggestion(doc_id, "ììœ¨ì£¼í–‰")

    print("[5] ì¬ì¶”ì²œ(ê°•í™” ë°˜ì˜):", brain.get_suggestions("ì", doc_id, top_n=6))
